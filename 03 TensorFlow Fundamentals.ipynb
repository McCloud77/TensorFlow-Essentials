{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurons as Logic Gates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic, classic activation function that we apply to neurons is a  sigmoid function:\n",
    "\n",
    "$$\n",
    "\\sigma = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$\\sigma$ ranges from (0, 1). When the input $x$ is negative, $\\sigma$ is close to 0. When $x$ is positive, $\\sigma$ is close to 1. At $x=0$, $\\sigma=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot The sigmoid function\n",
    "xs = np.linspace(-10, 10, num=100, dtype=np.float32)\n",
    "activation = sigmoid(xs)\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.plot(xs, activation)\n",
    "plt.plot(0,.5,'ro')\n",
    "\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='y')\n",
    "plt.axvline(x=0, color='y')\n",
    "plt.ylim([-0.1, 1.15]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Logic Gates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logic_gate(w1, w2, b):\n",
    "    '''\n",
    "    logic_gate is a function which returns a function.\n",
    "    the returned function take two args and (hopefully)\n",
    "    acts like a logic gate (and/or/not/etc.).  its behavior\n",
    "    is determined by w1,w2,b.  a longer, better name would be\n",
    "    make_twoarg_logic_gate_function\n",
    "    '''\n",
    "    def the_gate(x1, x2):\n",
    "        return sigmoid(w1 * x1 + w2 * x2 + b)\n",
    "    return the_gate\n",
    "\n",
    "def test(gate):\n",
    "    'Helper function to test out our weight functions.'\n",
    "    for a, b in it.product(range(2), repeat=2):\n",
    "        print(\"{}, {}: {}\".format(a, b, np.round(gate(a, b))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0: 0.0\n",
      "0, 1: 1.0\n",
      "1, 0: 1.0\n",
      "1, 1: 1.0\n"
     ]
    }
   ],
   "source": [
    "or_gate = logic_gate(20, 20, -10)\n",
    "test(or_gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0: 0.0\n",
      "0, 1: 0.0\n",
      "1, 0: 0.0\n",
      "1, 1: 1.0\n"
     ]
    }
   ],
   "source": [
    "and_gate = logic_gate(20,20,-30)\n",
    "test(and_gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0: 1.0\n",
      "0, 1: 0.0\n",
      "1, 0: 0.0\n",
      "1, 1: 0.0\n"
     ]
    }
   ],
   "source": [
    "nor_gate = logic_gate(-20,-20,20)\n",
    "test(nor_gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0: 1.0\n",
      "0, 1: 1.0\n",
      "1, 0: 1.0\n",
      "1, 1: 0.0\n"
     ]
    }
   ],
   "source": [
    "nand_gate = logic_gate(-20,-20,30)\n",
    "test(nand_gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0: 0.0\n",
      "0, 1: 1.0\n",
      "1, 0: 1.0\n",
      "1, 1: 0.0\n"
     ]
    }
   ],
   "source": [
    "# xor cannot be modeled by a single gate\n",
    "def xor_gate(a, b):\n",
    "    c = or_gate(a, b)\n",
    "    d = nand_gate(a, b)\n",
    "    return and_gate(c, d)\n",
    "test(xor_gate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaining together neurons can compose more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a Logic Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use TensorFlow to try and teach a model to learn the correct weights and bias by passing in our truth table as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_LogicGate:\n",
    "    def __init__(s):  \n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            \n",
    "            with tf.name_scope('inputs'):\n",
    "                s.x1 = tf.placeholder(tf.float32, name='x1')\n",
    "                s.x2 = tf.placeholder(tf.float32, name='x2')\n",
    "                s.label = tf.placeholder(tf.float32, name='label')\n",
    "                s.learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "                \n",
    "            with tf.name_scope('model'):\n",
    "                s.w1 = tf.Variable(tf.random_normal([]), name='w1')\n",
    "                s.w2 = tf.Variable(tf.random_normal([]), name='w2')\n",
    "                s.b = tf.Variable(0.0, dtype=tf.float32, name='b')\n",
    "                s.output = tf.nn.sigmoid(s.w1 * s.x1 + s.w2 * s.x2 + s.b)\n",
    "                \n",
    "            with tf.name_scope('loss'):\n",
    "                s.loss = tf.reduce_mean(tf.square(s.output - s.label), name='mse')\n",
    "                correct = tf.equal(tf.round(s.output), s.label)\n",
    "                s.accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "                \n",
    "            with tf.name_scope('train'):\n",
    "                s.train = tf.train.GradientDescentOptimizer(s.learning_rate).minimize(s.loss)\n",
    "                \n",
    "            s.init = tf.global_variables_initializer()\n",
    "            \n",
    "        s.session = tf.Session(graph=graph)\n",
    "        s.session.run(s.init)\n",
    "        \n",
    "    def fit(s, train_dict):\n",
    "        loss, acc, _ = s.session.run([s.loss, s.accuracy, s.train], feed_dict=train_dict)\n",
    "        return loss, acc\n",
    "\n",
    "    def predict(s, test_dict):\n",
    "        return s.session.run([s.w1, s.w2, s.b, s.output], feed_dict=test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up train dict with the results of an and table\n",
    "and_table = np.array([[0,0,0],\n",
    "                      [1,0,0],\n",
    "                      [0,1,0],\n",
    "                      [1,1,1]])\n",
    "\n",
    "logic_model = TF_LogicGate()\n",
    "\n",
    "train_dict={logic_model.x1:    and_table[:,0],\n",
    "            logic_model.x2:    and_table[:,1],\n",
    "            logic_model.label: and_table[:,2], \n",
    "            logic_model.learning_rate: 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "loss: 0.3586837649345398\taccuracy: 0.75\n",
      "loss: 0.25024935603141785\taccuracy: 0.75\n",
      "loss: 0.24990376830101013\taccuracy: 0.75\n",
      "loss: 0.24955244362354279\taccuracy: 0.75\n",
      "loss: 0.24527019262313843\taccuracy: 0.75\n",
      "loss: 0.01359378919005394\taccuracy: 1.0\n",
      "loss: 0.005986128933727741\taccuracy: 1.0\n",
      "loss: 0.0037496830336749554\taccuracy: 1.0\n",
      "loss: 0.0027070427313447\taccuracy: 1.0\n",
      "loss: 0.0021094896364957094\taccuracy: 1.0\n",
      "loss: 0.001724359579384327\taccuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"training\")\n",
    "for i in range(10000):\n",
    "    loss, acc = logic_model.fit(train_dict)\n",
    "    if i % 1000 == 0:\n",
    "        print('loss: {}\\taccuracy: {}'.format(loss, acc))\n",
    "print('loss: {}\\taccuracy: {}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {logic_model.x1: and_table[:,0], #[0.0, 1.0, 0.0, 1.0],\n",
    "             logic_model.x2: and_table[:,1]} # [0.0, 0.0, 1.0, 1.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n",
      "\n",
      "Learned weight for w1:\t 5.930612564086914\n",
      "Learned weight for w2:\t 5.930612564086914\n",
      "Learned weight for bias: -8.986828804016113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"testing\")\n",
    "w1_val, w2_val, b_val, out = logic_model.predict(test_dict)\n",
    "print('Learned weight for w1:\\t {}'.format(w1_val))\n",
    "print('Learned weight for w2:\\t {}'.format(w2_val))\n",
    "print('Learned weight for bias: {}\\n'.format(b_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0: 0.0\n",
      "0, 1: 0.0\n",
      "1, 0: 0.0\n",
      "1, 1: 1.0\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "for i in [0, 1]:\n",
    "    for j in [0, 1]:\n",
    "        print('{}, {}: {}'.format(i, j, np.round(out[idx])))\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning an XOR Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compose a two stage model, we can learn the XOR gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XOR_Graph:\n",
    "    def __init__(self):\n",
    "        # Create an empty Graph to place our operations in\n",
    "        xor_graph = tf.Graph()\n",
    "        with xor_graph.as_default():\n",
    "            # Placeholder inputs for our a, b, and label training data\n",
    "            self.x1 = tf.placeholder(tf.float32)\n",
    "            self.x2 = tf.placeholder(tf.float32)\n",
    "            self.label = tf.placeholder(tf.float32)\n",
    "\n",
    "            # A placeholder for our learning rate, so we can adjust it\n",
    "            self.learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "            Var = tf.Variable; rn = tf.random_normal\n",
    "            self.weights = [[Var(rn([])), Var(rn([]))],\n",
    "                            [Var(rn([])), Var(rn([]))],\n",
    "                            [Var(rn([])), Var(rn([]))]]\n",
    "            self.biases = [Var(0.0, dtype=tf.float32),\n",
    "                           Var(0.0, dtype=tf.float32),\n",
    "                           Var(0.0, dtype=tf.float32)]\n",
    "            sig1 = tf.nn.sigmoid(self.x1 * self.weights[0][0] + \n",
    "                                 self.x2 * self.weights[0][1] + \n",
    "                                 self.biases[0])\n",
    "            sig2 = tf.nn.sigmoid(self.x1 * self.weights[1][0] + \n",
    "                                 self.x2 * self.weights[1][1] + \n",
    "                                 self.biases[1])\n",
    "            self.output = tf.nn.sigmoid(sig1 * self.weights[2][0] + \n",
    "                                        sig2 * self.weights[2][1] + \n",
    "                                        self.biases[2])\n",
    "\n",
    "            # We'll use the mean of squared errors as our loss function \n",
    "            self.loss = tf.reduce_mean(tf.square(self.output - self.label))\n",
    "            \n",
    "            # create a gradient descent training operation \n",
    "            # and an initialization operation\n",
    "            gdo = tf.train.GradientDescentOptimizer\n",
    "            self.train = gdo(self.learning_rate).minimize(self.loss)\n",
    "            \n",
    "            correct = tf.equal(tf.round(self.output), self.label)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        \n",
    "            init = tf.global_variables_initializer()\n",
    "        \n",
    "        self.sess = tf.Session(graph=xor_graph)\n",
    "        self.sess.run(init)        \n",
    "\n",
    "    def fit(self, train_dict):\n",
    "        loss, acc, _ = self.sess.run([self.loss, self.accuracy, self.train], \n",
    "                                     train_dict)\n",
    "        return loss, acc\n",
    "        \n",
    "    def predict(self, test_dict):\n",
    "        all_trained = (self.weights[0] + [self.biases[0]] +\n",
    "                       self.weights[1] + [self.biases[1]] +\n",
    "                       self.weights[2] + [self.biases[2]])\n",
    "        return self.sess.run(all_trained + [self.output], test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "loss: 0.29703134298324585\taccuracy: 0.5\n",
      "loss: 0.19086095690727234\taccuracy: 0.75\n",
      "loss: 0.06518189609050751\taccuracy: 1.0\n",
      "loss: 0.012577787972986698\taccuracy: 1.0\n",
      "loss: 0.006106953602284193\taccuracy: 1.0\n",
      "loss: 0.003922253847122192\taccuracy: 1.0\n",
      "loss: 0.0028577803168445826\taccuracy: 1.0\n",
      "loss: 0.0022355434484779835\taccuracy: 1.0\n",
      "loss: 0.001829994609579444\taccuracy: 1.0\n",
      "loss: 0.001545838313177228\taccuracy: 1.0\n",
      "loss: 0.0013363773468881845\taccuracy: 1.0\n",
      "testing\n",
      "[6.870407, 6.8782935, -3.057933, -4.8926725, -4.8940954, 7.3238125, 7.7410927, 7.9591722, -11.53266]\n",
      "results\n",
      "Learned weights/bias (L1): [ 6.870407   6.8782935 -3.057933 ]\n",
      "Learned weights/bias (L2): [-4.8926725 -4.8940954  7.3238125]\n",
      "Learned weights/bias (L3): [  7.7410927   7.9591722 -11.53266  ]\n",
      "Testing Table:\n",
      "[[0 0 0]\n",
      " [1 0 1]\n",
      " [0 1 1]\n",
      " [1 1 0]]\n",
      "Correct? True\n"
     ]
    }
   ],
   "source": [
    "xor_table = np.array([[0,0,0],\n",
    "                      [1,0,1],\n",
    "                      [0,1,1],\n",
    "                      [1,1,0]])\n",
    "\n",
    "logic_model = XOR_Graph()\n",
    "\n",
    "train_dict={logic_model.x1:    xor_table[:,0],\n",
    "            logic_model.x2:    xor_table[:,1],\n",
    "            logic_model.label: xor_table[:,2],\n",
    "            logic_model.learning_rate: 0.5}\n",
    "\n",
    "print(\"training\")\n",
    "for i in range(10000):\n",
    "    loss, acc = logic_model.fit(train_dict)\n",
    "    if i % 1000 == 0:\n",
    "        print('loss: {}\\taccuracy: {}'.format(loss, acc))\n",
    "print('loss: {}\\taccuracy: {}'.format(loss, acc))\n",
    "            \n",
    "print(\"testing\")\n",
    "test_dict = {logic_model.x1: xor_table[:,0], \n",
    "             logic_model.x2: xor_table[:,1]}\n",
    "results = logic_model.predict(test_dict)\n",
    "wb_lrn, predictions = results[:-1], results[-1]\n",
    "\n",
    "print(wb_lrn)\n",
    "wb_lrn = np.array(wb_lrn).reshape(3,3)\n",
    "\n",
    "# combine the predictions with the inputs and clean up the data\n",
    "# round it and convert to unsigned 8 bit ints\n",
    "out_table = np.column_stack((xor_table[:,[0,1]], \n",
    "                             predictions)).round().astype(np.uint8)\n",
    "\n",
    "print(\"results\")\n",
    "print('Learned weights/bias (L1):', wb_lrn[0])\n",
    "print('Learned weights/bias (L2):', wb_lrn[1])\n",
    "print('Learned weights/bias (L3):', wb_lrn[2])\n",
    "print('Testing Table:')\n",
    "print(out_table)\n",
    "print(\"Correct?\", np.allclose(xor_table, out_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Example Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now that we've worked with some primitive models, let's take a look at something a bit closer to what we'll work with moving forward:  an actual neural network.\n",
    "\n",
    "The following model accepts a 100 dimensional input, has a hidden layer depth of 300, and an output layer depth of 50. We use a sigmoid activation function for the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_graph = tf.Graph()\n",
    "with nn1_graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "    y = tf.placeholder(tf.float32, shape=[None])\n",
    "    \n",
    "    with tf.name_scope('hidden1'):\n",
    "        w = tf.Variable(tf.truncated_normal([100, 300]), name='W')\n",
    "        b = tf.Variable(tf.zeros([300]), name='b')\n",
    "        z = tf.matmul(x, w) + b\n",
    "        a = tf.nn.sigmoid(z)\n",
    "    \n",
    "    with tf.name_scope('output'):\n",
    "        w = tf.Variable(tf.truncated_normal([300, 50]), name='W')\n",
    "        b = tf.Variable(tf.zeros([50]), name='b')\n",
    "        z = tf.matmul(a, w) + b\n",
    "        output = z\n",
    "    \n",
    "    with tf.name_scope('global_step'):\n",
    "        global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        inc_step = tf.assign_add(global_step, 1, name='increment_step')\n",
    "    \n",
    "    with tf.name_scope('summaries'):\n",
    "        for var in tf.trainable_variables():\n",
    "            hist_summary = tf.summary.histogram(var.op.name, var)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    init = tf.global_variables_initializer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
